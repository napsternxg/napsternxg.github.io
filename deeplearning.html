<h1>Bibliography</h1>
<p>Some resources on deep learning I have enjoyed reading</p>
<h2>Papers</h2>
<div id="bibtex_display"></div>

<h2>Blogs & Websites</h2>
<p>I have personally got excited about deep learning and neural networks by reading the work on websites and trying out their examples.</p>
<ul>
	<li><a href="http://colah.github.io">Chris Olah's Blog</a> - One of the best explanations of all things neural networks.</li>
	<li><a href="http://karpathy.github.io">Andrej Karpathy's Blog</a> - Great resource for code implementations and intuitive understanding of neural networks in image and text.</li>
	<li><a href="http://cs231n.github.io/">CS231n - Lecture Notes</a> - Another wonderful resource partly prepared by Andrej Karpathy.</li>
	<li><a href="http://coursera.org/course/neuralnets">Geoffrey Hinton's Coursera class on Neural Networks</a> - In depth and comprehensive.</li>
	<li><a href="http://neuralnetworksanddeeplearning.com">Michael Nielsen - Online Free Book</a> - You can use it like a text book.</li>
	<li><a href="http://reddit.com/r/MachineLearning/">MachineLearning Subreddit</a> - For all the latest news.</li>
</ul>

<h2>Introductory presentation</h2>
<iframe src="//www.slideshare.net/slideshow/embed_code/key/pxgoLAand9HVyx" width="425" height="355" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/shubhanshu/lets-learn-deep" title="Let’s learn deep" target="_blank">Let’s learn deep</a> </strong> from <strong><a href="//www.slideshare.net/shubhanshu" target="_blank">Shubhanshu Mishra</a></strong> </div>


<textarea id="bibtex_input" style="display:none;">
@article{Collobert2011,
abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.},
author = {Collobert, Ronan and Weston, Jason and Bottou, Leon and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
file = {:E$\backslash$:/Box/Box Sync/Research/Papers/Library/Collobert et al. - 2011 - Natural Language Processing (Almost) from Scratch.pdf:pdf},
issn = {1532-4435},
journal = {The Journal of Machine Learning Research},
mendeley-groups = {NLP},
month = feb,
pages = {2493--2537},
publisher = {JMLR.org},
title = {{Natural Language Processing (Almost) from Scratch}},
url = {http://dl.acm.org/citation.cfm?id=1953048.2078186 https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH http://info.usherbrooke.ca/hlarochelle/cours/ift725\_A2013/contenu.html},
volume = {12},
year = {2011}
}
@inproceedings{Mikolov2013a,
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S. and Dean, Jeff},
booktitle = {Advances in Neural Information Processing Systems},
file = {:C$\backslash$:/Users/Shubhanshu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases and their Compositionality.pdf:pdf},
mendeley-groups = {NLP},
pages = {3111--3119},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
url = {http://papers.nips.cc/paper/5021-di},
year = {2013}
}
@article{Schmidhuber2014,
abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
author = {Schmidhuber, Jurgen},
doi = {10.1016/j.neunet.2014.09.003},
file = {:C$\backslash$:/Users/Shubhanshu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schmidhuber - 2014 - Deep learning in neural networks An overview(2).pdf:pdf},
issn = {08936080},
journal = {Neural Networks},
keywords = {Deep learning,Evolutionary computation,Reinforcement learning,Supervised learning,Unsupervised learning},
mendeley-groups = {Deep Learning},
month = oct,
pages = {85--117},
title = {{Deep learning in neural networks: An overview}},
url = {http://www.sciencedirect.com/science/article/pii/S0893608014002135},
volume = {61},
year = {2014}
}
@article{LeCun2015,
author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
issn = {0028-0836},
journal = {Nature},
language = {en},
mendeley-groups = {Deep Learning},
month = may,
number = {7553},
pages = {436--444},
publisher = {Nature Publishing Group},
title = {{Deep learning}},
url = {http://www.nature.com/doifinder/10.1038/nature14539},
volume = {521},
year = {2015}
}
@article{Socher2013,
abstract = {Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80\% up to 85.4\%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7\%, an improvement of 9.7\% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.},
author = {Socher, Richard and Perelygin, Alex and Wu, Jy},
file = {:E$\backslash$:/Box/Box Sync/Research/Papers/Library/Socher, Perelygin, Wu - 2013 - Recursive deep models for semantic compositionality over a sentiment treebank.pdf:pdf},
journal = {Conference on Empirical Methods in Natural Language Processing (EMNLP 2013)},
mendeley-groups = {NLP},
pages = {1631--1642},
title = {{Recursive deep models for semantic compositionality over a sentiment treebank}},
url = {http://nlp.stanford.edu/~socherr/EMNLP2013\_RNTN.pdf$\backslash$nhttp://www.aclweb.org/anthology/D13-1170$\backslash$nhttp://aclweb.org/supplementals/D/D13/D13-1170.Attachment.pdf$\backslash$nhttp://oldsite.aclweb.org/anthology-new/D/D13/D13-1170.pdf},
year = {2013}
}
@inproceedings{hinton1986learning,
  title={Learning distributed representations of concepts},
  author={Hinton, Geoffrey E},
  booktitle={Proceedings of the eighth annual conference of the cognitive science society},
  volume={1},
  pages={12},
  year={1986},
  organization={Amherst, MA}
}
</textarea>
<script type="text/javascript" src="http://ajax.googleapis.com/ajax/libs/jquery/1.4.2/jquery.min.js"></script>
<script type="text/javascript" src="https://cdn.rawgit.com/napsternxg/bibtex-js/b2148a5d5a64275fb8fd3f1575eccab145588db5/src/bibtex_js.js"></script>